[vlm]
path = "/home/<user>/llama.cpp/build/bin/llama-mtmd-cli"
model = "/home/<user>/models/llava-v1.5-7b-Q4_K_M.gguf"
mmproj = "/home/<user>/models/llava-v1.5-7b-mmproj-model-f16.gguf"
chat_template = "vicuna"
ctx_size = 4096
